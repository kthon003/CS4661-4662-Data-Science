{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d26b8057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from  keras.wrappers.scikit_learn  import  KerasClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from PIL import Image\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from scipy import ndimage\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from keras.preprocessing.image import ImageDataGenerator, img_to_array, array_to_img, load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e024049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test  = pd.read_csv('/Users/Michael Scott/Desktop/shopee-product-matching/test.csv')\n",
    "df_train  = pd.read_csv('/Users/Michael Scott/Desktop/shopee-product-matching/train.csv')\n",
    "df_trains  = pd.read_csv('/Users/Michael Scott/Desktop/shopee-product-matching/train_sample.csv')\n",
    "df_test.head()\n",
    "feature_cols = ['posting_id','image','image_phash','title']\n",
    "y = df_trains['label_group']\n",
    "X = df_train[feature_cols]\n",
    "#y_trains = df_trains['label_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f1abcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = []\n",
    "y_train = []\n",
    "image_width = 640\n",
    "image_height = 640\n",
    "i=0\n",
    "image_path = '/Users/Michael Scott/Desktop/shopee-product-matching/train_samples/' + df_trains['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58d796e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[168  98 108 ... 114 134 255]\n"
     ]
    }
   ],
   "source": [
    "for m in range(len(df_trains)):\n",
    "    img = Image.open(image_path[m]).convert('RGBA')\n",
    "    img = img.resize((image_width, image_height))\n",
    "    arr = np.array(img)\n",
    "    flatten_arr = arr.ravel()\n",
    "    train_files.append(flatten_arr)\n",
    "\n",
    "print(train_files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "510bc0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = preprocessing.scale(train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f94e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(normalized, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7e187f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest accuracy: \n",
      "0.05405405405405406\n",
      "Random Forest AUC: \n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael Scott\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "from   sklearn.ensemble    import    RandomForestClassifier\n",
    "\n",
    "my_RandomForest = RandomForestClassifier(n_estimators = 29, bootstrap = True, random_state=2)\n",
    "my_RandomForest.fit(X_train, y_train)\n",
    "y_predict_rf = my_RandomForest.predict(X_test)\n",
    "\n",
    "score_rf = accuracy_score(y_test, y_predict_rf)\n",
    "\n",
    "y_predict_rf_proba = my_RandomForest.predict_proba(X_test)\n",
    "\n",
    "fpr4, tpr4, thresholds = metrics.roc_curve(y_test, y_predict_rf_proba[:,1], pos_label=1)\n",
    "rf_AUC = metrics.auc(fpr4, tpr4)\n",
    "print(\"Random Forest accuracy: \")\n",
    "print(score_rf)\n",
    "print(\"Random Forest AUC: \")\n",
    "print(rf_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07477760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael Scott\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06:58:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "XGBoost accuracy: \n",
      "0.05405405405405406\n",
      "XGBoost AUC: \n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "from   xgboost   import   XGBClassifier\n",
    "\n",
    "my_XGBoost = XGBClassifier(n_estimators = 29,random_state=2)\n",
    "my_XGBoost.fit(X_train, y_train)\n",
    "y_XGBoost_predict = my_XGBoost.predict(X_test)\n",
    "xG_score = accuracy_score(y_test, y_XGBoost_predict)\n",
    "\n",
    "y_predict_xg_proba = my_XGBoost.predict_proba(X_test)\n",
    "\n",
    "fpr3, tpr3, thresholds = metrics.roc_curve(y_test, y_predict_xg_proba[:,1], pos_label=1)\n",
    "\n",
    "xg_AUC=metrics.auc(fpr3, tpr3)\n",
    "\n",
    "\n",
    "print(\"XGBoost accuracy: \")\n",
    "print(xG_score)\n",
    "print(\"XGBoost AUC: \")\n",
    "print(xg_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d20efffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada accuracy: \n",
      "0.0\n",
      "Ada AUC: \n",
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael Scott\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:659: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "from   sklearn.ensemble   import     AdaBoostClassifier\n",
    "\n",
    "my_AdaBoost = AdaBoostClassifier(n_estimators = 29,random_state=2)\n",
    "my_AdaBoost.fit(X_train, y_train)\n",
    "y_AdaBoost_predict = my_AdaBoost.predict(X_test)\n",
    "adaBoost_score = accuracy_score(y_test, y_AdaBoost_predict)\n",
    "\n",
    "y_predict_ada_proba = my_AdaBoost.predict_proba(X_test)\n",
    "\n",
    "fpr2, tpr2, thresholds = metrics.roc_curve(y_test, y_predict_ada_proba[:,1], pos_label=1)\n",
    "\n",
    "ada_AUC = metrics.auc(fpr2, tpr2)\n",
    "\n",
    "print(\"Ada accuracy: \")\n",
    "print(adaBoost_score)\n",
    "print(\"Ada AUC: \")\n",
    "print(ada_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d68e1b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "my_ANN = MLPClassifier(hidden_layer_sizes=(30,), activation= 'logistic', \n",
    "                       solver='adam', alpha=1, random_state=1, \n",
    "                       learning_rate_init = 0.02, verbose=True, tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eda0805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_ANN = MLPClassifier(hidden_layer_sizes=(30,), activation= 'logistic', \n",
    "                       solver='adam', alpha=1, random_state=1, \n",
    "                       learning_rate_init = 0.02, verbose=True, tol=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34125d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 32.06585749\n",
      "Iteration 2, loss = 92.64918108\n",
      "Iteration 3, loss = 96.03986236\n",
      "Iteration 4, loss = 62.41232448\n",
      "Iteration 5, loss = 38.59468300\n",
      "Iteration 6, loss = 25.94600396\n",
      "Iteration 7, loss = 22.85831817\n",
      "Iteration 8, loss = 24.38198026\n",
      "Iteration 9, loss = 23.46992652\n",
      "Iteration 10, loss = 22.01164526\n",
      "Iteration 11, loss = 20.38339132\n",
      "Iteration 12, loss = 17.43001335\n",
      "Iteration 13, loss = 15.36185030\n",
      "Iteration 14, loss = 14.79178218\n",
      "Iteration 15, loss = 14.20033917\n",
      "Iteration 16, loss = 13.61720279\n",
      "Iteration 17, loss = 12.93795538\n",
      "Iteration 18, loss = 11.88496202\n",
      "Iteration 19, loss = 11.18501865\n",
      "Iteration 20, loss = 11.03283696\n",
      "Iteration 21, loss = 10.64494887\n",
      "Iteration 22, loss = 9.85867270\n",
      "Iteration 23, loss = 9.45714477\n",
      "Iteration 24, loss = 9.46083705\n",
      "Iteration 25, loss = 9.26219148\n",
      "Iteration 26, loss = 8.63140335\n",
      "Iteration 27, loss = 8.43274301\n",
      "Iteration 28, loss = 8.80794658\n",
      "Iteration 29, loss = 8.84528716\n",
      "Iteration 30, loss = 8.26583490\n",
      "Iteration 31, loss = 8.17741184\n",
      "Iteration 32, loss = 8.81652684\n",
      "Iteration 33, loss = 9.40573442\n",
      "Iteration 34, loss = 9.49148003\n",
      "Iteration 35, loss = 9.06381767\n",
      "Iteration 36, loss = 9.17340018\n",
      "Iteration 37, loss = 10.31467946\n",
      "Iteration 38, loss = 11.39051406\n",
      "Iteration 39, loss = 11.56492032\n",
      "Iteration 40, loss = 10.76887278\n",
      "Iteration 41, loss = 10.31249675\n",
      "Iteration 42, loss = 10.66542592\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=1, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(30,), learning_rate='constant',\n",
       "              learning_rate_init=0.02, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ANN.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9bf9bb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael Scott\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.44373411\n",
      "Iteration 2, loss = 565.67667468\n",
      "Iteration 3, loss = 841.09722642\n",
      "Iteration 4, loss = 1045.86870955\n",
      "Iteration 5, loss = 1099.05029381\n",
      "Iteration 6, loss = 971.91029018\n",
      "Iteration 7, loss = 773.07151725\n",
      "Iteration 8, loss = 596.15099977\n",
      "Iteration 9, loss = 472.19546468\n",
      "Iteration 10, loss = 379.61025823\n",
      "Iteration 11, loss = 296.10421918\n",
      "Iteration 12, loss = 225.37718079\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 33.10538757\n",
      "Iteration 2, loss = 92.85313656\n",
      "Iteration 3, loss = 94.45810610\n",
      "Iteration 4, loss = 60.16113701\n",
      "Iteration 5, loss = 36.78387143\n",
      "Iteration 6, loss = 25.40670392\n",
      "Iteration 7, loss = 22.68952919\n",
      "Iteration 8, loss = 24.50647292\n",
      "Iteration 9, loss = 23.66092102\n",
      "Iteration 10, loss = 21.24923776\n",
      "Iteration 11, loss = 19.60355008\n",
      "Iteration 12, loss = 17.67858874\n",
      "Iteration 13, loss = 15.84255730\n",
      "Iteration 14, loss = 14.00352312\n",
      "Iteration 15, loss = 12.17986342\n",
      "Iteration 16, loss = 11.64774598\n",
      "Iteration 17, loss = 11.57334635\n",
      "Iteration 18, loss = 10.91535857\n",
      "Iteration 19, loss = 10.16571389\n",
      "Iteration 20, loss = 9.52621806\n",
      "Iteration 21, loss = 9.08940483\n",
      "Iteration 22, loss = 8.92618203\n",
      "Iteration 23, loss = 8.61868485\n",
      "Iteration 24, loss = 8.41417392\n",
      "Iteration 25, loss = 8.57284992\n",
      "Iteration 26, loss = 8.57372998\n",
      "Iteration 27, loss = 8.45426129\n",
      "Iteration 28, loss = 9.00679460\n",
      "Iteration 29, loss = 9.28759485\n",
      "Iteration 30, loss = 9.61393295\n",
      "Iteration 31, loss = 9.72468938\n",
      "Iteration 32, loss = 9.51124068\n",
      "Iteration 33, loss = 9.83997714\n",
      "Iteration 34, loss = 10.02894580\n",
      "Iteration 35, loss = 9.69451594\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 32.38247405\n",
      "Iteration 2, loss = 92.73724679\n",
      "Iteration 3, loss = 95.50925367\n",
      "Iteration 4, loss = 61.59397298\n",
      "Iteration 5, loss = 37.77467557\n",
      "Iteration 6, loss = 25.25080399\n",
      "Iteration 7, loss = 22.41759858\n",
      "Iteration 8, loss = 24.85498715\n",
      "Iteration 9, loss = 24.42105560\n",
      "Iteration 10, loss = 22.31740356\n",
      "Iteration 11, loss = 20.07592864\n",
      "Iteration 12, loss = 17.26617945\n",
      "Iteration 13, loss = 15.58106326\n",
      "Iteration 14, loss = 14.65814102\n",
      "Iteration 15, loss = 13.78940447\n",
      "Iteration 16, loss = 13.04774545\n",
      "Iteration 17, loss = 12.18805679\n",
      "Iteration 18, loss = 11.72559364\n",
      "Iteration 19, loss = 11.42703836\n",
      "Iteration 20, loss = 10.89653065\n",
      "Iteration 21, loss = 10.54661774\n",
      "Iteration 22, loss = 10.09427592\n",
      "Iteration 23, loss = 9.50528383\n",
      "Iteration 24, loss = 9.16445078\n",
      "Iteration 25, loss = 9.08973896\n",
      "Iteration 26, loss = 9.08614390\n",
      "Iteration 27, loss = 8.75433001\n",
      "Iteration 28, loss = 8.55043371\n",
      "Iteration 29, loss = 9.25991373\n",
      "Iteration 30, loss = 9.11999717\n",
      "Iteration 31, loss = 8.51922464\n",
      "Iteration 32, loss = 9.86980473\n",
      "Iteration 33, loss = 12.09941606\n",
      "Iteration 34, loss = 12.30964646\n",
      "Iteration 35, loss = 10.57335110\n",
      "Iteration 36, loss = 9.43808664\n",
      "Iteration 37, loss = 9.87420980\n",
      "Iteration 38, loss = 10.85020143\n",
      "Iteration 39, loss = 10.81213985\n",
      "Iteration 40, loss = 10.37920268\n",
      "Iteration 41, loss = 10.14214759\n",
      "Iteration 42, loss = 10.01808937\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_list = cross_val_score(my_ANN, X_train, y_train, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "968f3646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 5.10484941\n",
      "Iteration 2, loss = 66.42639908\n",
      "Iteration 3, loss = 108.21185087\n",
      "Iteration 4, loss = 142.63797446\n",
      "Iteration 5, loss = 155.53106548\n",
      "Iteration 6, loss = 146.22521884\n",
      "Iteration 7, loss = 127.06773060\n",
      "Iteration 8, loss = 107.30431702\n",
      "Iteration 9, loss = 89.59288700\n",
      "Iteration 10, loss = 72.85971127\n",
      "Iteration 11, loss = 57.26337846\n",
      "Iteration 12, loss = 44.61543209\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='logistic', alpha=1, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(30,), learning_rate='constant',\n",
       "              learning_rate_init=0.02, max_iter=200, momentum=0.9,\n",
       "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "              random_state=1, shuffle=True, solver='adam', tol=0.0001,\n",
       "              validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ANN.fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d94ca6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael Scott\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=2.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 3.83845115\n",
      "Iteration 2, loss = 1931.24517141\n",
      "Iteration 3, loss = 1859.17200237\n",
      "Iteration 4, loss = 1693.87560665\n",
      "Iteration 5, loss = 1507.03368828\n",
      "Iteration 6, loss = 1079.34850420\n",
      "Iteration 7, loss = 638.16044547\n",
      "Iteration 8, loss = 408.68494959\n",
      "Iteration 9, loss = 420.25414976\n",
      "Iteration 10, loss = 514.70111987\n",
      "Iteration 11, loss = 537.03400587\n",
      "Iteration 12, loss = 470.55015898\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 5.10815989\n",
      "Iteration 2, loss = 68.68781864\n",
      "Iteration 3, loss = 111.17739197\n",
      "Iteration 4, loss = 145.67832748\n",
      "Iteration 5, loss = 157.91952889\n",
      "Iteration 6, loss = 147.35180833\n",
      "Iteration 7, loss = 127.00434834\n",
      "Iteration 8, loss = 106.44469723\n",
      "Iteration 9, loss = 88.32524322\n",
      "Iteration 10, loss = 71.29662001\n",
      "Iteration 11, loss = 55.45469493\n",
      "Iteration 12, loss = 42.81217792\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "accuracy_list = cross_val_score(my_ANN, X_test, y_test, cv=2, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11cb7b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the testing set is :\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "accuracy_cv = accuracy_list.mean()\n",
    "print('The accuracy on the testing set is :')\n",
    "print(accuracy_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3fa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_decisiontree = DecisionTreeClassifier(random_state=2)\n",
    "my_decisiontree.fit(X_train, y_train)\n",
    "y_predict_dt = my_decisiontree.predict(X_test)\n",
    "score_dt = accuracy_score(y_test, y_predict_dt)\n",
    "print('Decision tree accuracy: ')\n",
    "print(score_dt)\n",
    "\n",
    "y_predict_dt_proba = my_decisiontree.predict_proba(X_test)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_predict_dt_proba[:,1], pos_label=1)\n",
    "\n",
    "dt_AUC = metrics.auc(fpr, tpr)\n",
    "print('Decision tree AUC: ')\n",
    "print(dt_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f985ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_layer_sizes': [(5,), (10,), (15,), (20,), (25,), (30,), (35,), (40,), (45,), (50,), (55,), (60,), (65,), (70,), (75,), (80,), (85,), (90,), (95,), (100,), (105,), (110,), (115,), (120,), (125,), (130,), (135,), (140,), (145,), (150,), (155,), (160,), (165,), (170,), (175,), (180,), (185,), (190,), (195,), (200,), (205,), (210,), (215,), (220,), (225,), (230,), (235,), (240,), (245,), (250,)]} \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael Scott\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:657: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=4.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.69980139\n",
      "Iteration 2, loss = 86.38193756\n",
      "Iteration 3, loss = 148.07131091\n",
      "Iteration 4, loss = 207.64681325\n",
      "Iteration 5, loss = 242.64662251\n",
      "Iteration 6, loss = 243.77908241\n",
      "Iteration 7, loss = 225.05660870\n",
      "Iteration 8, loss = 200.09883864\n",
      "Iteration 9, loss = 174.81970052\n",
      "Iteration 10, loss = 148.35310669\n",
      "Iteration 11, loss = 120.15829773\n",
      "Iteration 12, loss = 93.19813062\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10.35019162\n",
      "Iteration 2, loss = 20.69821577\n",
      "Iteration 3, loss = 21.38639607\n",
      "Iteration 4, loss = 15.64762224\n",
      "Iteration 5, loss = 11.39589393\n",
      "Iteration 6, loss = 9.17757846\n",
      "Iteration 7, loss = 8.63697344\n",
      "Iteration 8, loss = 8.82143821\n",
      "Iteration 9, loss = 8.80300628\n",
      "Iteration 10, loss = 8.62981932\n",
      "Iteration 11, loss = 8.19732906\n",
      "Iteration 12, loss = 7.51421692\n",
      "Iteration 13, loss = 7.06580152\n",
      "Iteration 14, loss = 6.85993204\n",
      "Iteration 15, loss = 6.67681577\n",
      "Iteration 16, loss = 6.95766843\n",
      "Iteration 17, loss = 7.15634206\n",
      "Iteration 18, loss = 6.93488556\n",
      "Iteration 19, loss = 6.53718643\n",
      "Iteration 20, loss = 6.33122326\n",
      "Iteration 21, loss = 6.37636195\n",
      "Iteration 22, loss = 6.36976348\n",
      "Iteration 23, loss = 6.31688329\n",
      "Iteration 24, loss = 6.29487455\n",
      "Iteration 25, loss = 6.30059372\n",
      "Iteration 26, loss = 6.22066425\n",
      "Iteration 27, loss = 6.17288039\n",
      "Iteration 28, loss = 6.14041660\n",
      "Iteration 29, loss = 6.69740199\n",
      "Iteration 30, loss = 7.62140547\n",
      "Iteration 31, loss = 7.28823961\n",
      "Iteration 32, loss = 6.68430198\n",
      "Iteration 33, loss = 7.93859752\n",
      "Iteration 34, loss = 9.02134799\n",
      "Iteration 35, loss = 8.46292537\n",
      "Iteration 36, loss = 7.24874572\n",
      "Iteration 37, loss = 6.61139313\n",
      "Iteration 38, loss = 6.67660725\n",
      "Iteration 39, loss = 6.83362659\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10.23185312\n",
      "Iteration 2, loss = 20.48253545\n",
      "Iteration 3, loss = 21.14501361\n",
      "Iteration 4, loss = 15.45691838\n",
      "Iteration 5, loss = 11.29891339\n",
      "Iteration 6, loss = 9.09523139\n",
      "Iteration 7, loss = 8.62830504\n",
      "Iteration 8, loss = 9.23379838\n",
      "Iteration 9, loss = 9.30494204\n",
      "Iteration 10, loss = 8.80075417\n",
      "Iteration 11, loss = 8.21607699\n",
      "Iteration 12, loss = 7.60690401\n",
      "Iteration 13, loss = 7.26477684\n",
      "Iteration 14, loss = 7.08726942\n",
      "Iteration 15, loss = 6.98521852\n",
      "Iteration 16, loss = 7.05966922\n",
      "Iteration 17, loss = 6.89768759\n",
      "Iteration 18, loss = 6.56314494\n",
      "Iteration 19, loss = 6.47893372\n",
      "Iteration 20, loss = 6.46237717\n",
      "Iteration 21, loss = 6.43177293\n",
      "Iteration 22, loss = 6.48222575\n",
      "Iteration 23, loss = 6.57795217\n",
      "Iteration 24, loss = 6.38210925\n",
      "Iteration 25, loss = 6.29725198\n",
      "Iteration 26, loss = 6.43420120\n",
      "Iteration 27, loss = 6.38970334\n",
      "Iteration 28, loss = 6.38955865\n",
      "Iteration 29, loss = 6.45858865\n",
      "Iteration 30, loss = 6.38603104\n",
      "Iteration 31, loss = 6.33420796\n",
      "Iteration 32, loss = 6.55179617\n",
      "Iteration 33, loss = 6.58679788\n",
      "Iteration 34, loss = 6.52085335\n",
      "Iteration 35, loss = 6.87498907\n",
      "Iteration 36, loss = 6.92099850\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 10.20061177\n",
      "Iteration 2, loss = 20.17268148\n",
      "Iteration 3, loss = 20.46104668\n",
      "Iteration 4, loss = 14.68711371\n",
      "Iteration 5, loss = 10.71938244\n",
      "Iteration 6, loss = 8.74768061\n",
      "Iteration 7, loss = 8.55113802\n",
      "Iteration 8, loss = 9.21493593\n",
      "Iteration 9, loss = 9.16056892\n",
      "Iteration 10, loss = 8.76079675\n",
      "Iteration 11, loss = 8.39735004\n",
      "Iteration 12, loss = 7.88032241\n",
      "Iteration 13, loss = 7.51359519\n",
      "Iteration 14, loss = 7.52362264\n",
      "Iteration 15, loss = 7.47494182\n",
      "Iteration 16, loss = 7.22092503\n",
      "Iteration 17, loss = 6.85026894\n",
      "Iteration 18, loss = 6.72102316\n",
      "Iteration 19, loss = 6.60964318\n",
      "Iteration 20, loss = 6.44623381\n",
      "Iteration 21, loss = 6.32834888\n",
      "Iteration 22, loss = 6.27968726\n",
      "Iteration 23, loss = 6.21854440\n",
      "Iteration 24, loss = 6.24628232\n",
      "Iteration 25, loss = 6.53932048\n",
      "Iteration 26, loss = 6.78734221\n",
      "Iteration 27, loss = 6.64174624\n",
      "Iteration 28, loss = 6.37622622\n",
      "Iteration 29, loss = 6.23250254\n",
      "Iteration 30, loss = 6.35009907\n",
      "Iteration 31, loss = 6.41402579\n",
      "Iteration 32, loss = 6.28690012\n",
      "Iteration 33, loss = 6.32024206\n",
      "Iteration 34, loss = 6.80622264\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.93741102\n",
      "Iteration 2, loss = 170.36734688\n",
      "Iteration 3, loss = 284.76721740\n",
      "Iteration 4, loss = 390.90365486\n",
      "Iteration 5, loss = 448.31500963\n",
      "Iteration 6, loss = 440.82866668\n",
      "Iteration 7, loss = 397.67970974\n",
      "Iteration 8, loss = 346.49857307\n",
      "Iteration 9, loss = 298.61268993\n",
      "Iteration 10, loss = 251.34031756\n",
      "Iteration 11, loss = 202.57048506\n",
      "Iteration 12, loss = 157.04632598\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 14.89128137\n",
      "Iteration 2, loss = 34.57925594\n",
      "Iteration 3, loss = 34.94905271\n",
      "Iteration 4, loss = 23.46506045\n",
      "Iteration 5, loss = 15.78590457\n",
      "Iteration 6, loss = 12.17338186\n",
      "Iteration 7, loss = 11.63206073\n",
      "Iteration 8, loss = 12.51935500\n",
      "Iteration 9, loss = 12.33999357\n",
      "Iteration 10, loss = 11.33487405\n",
      "Iteration 11, loss = 10.37307480\n",
      "Iteration 12, loss = 9.69147851\n",
      "Iteration 13, loss = 9.31966426\n",
      "Iteration 14, loss = 8.88476098\n",
      "Iteration 15, loss = 8.23550953\n",
      "Iteration 16, loss = 7.73739010\n",
      "Iteration 17, loss = 7.75486669\n",
      "Iteration 18, loss = 7.79062886\n",
      "Iteration 19, loss = 7.47380061\n",
      "Iteration 20, loss = 7.08266443\n",
      "Iteration 21, loss = 6.88645431\n",
      "Iteration 22, loss = 6.79889027\n",
      "Iteration 23, loss = 6.70021754\n",
      "Iteration 24, loss = 6.62641764\n",
      "Iteration 25, loss = 6.54169748\n",
      "Iteration 26, loss = 6.49612146\n",
      "Iteration 27, loss = 6.68441709\n",
      "Iteration 28, loss = 6.79985374\n",
      "Iteration 29, loss = 6.72089060\n",
      "Iteration 30, loss = 6.73742731\n",
      "Iteration 31, loss = 6.92777205\n",
      "Iteration 32, loss = 7.38962861\n",
      "Iteration 33, loss = 7.46589336\n",
      "Iteration 34, loss = 7.33776819\n",
      "Iteration 35, loss = 7.55938412\n",
      "Iteration 36, loss = 7.41076401\n",
      "Iteration 37, loss = 7.06178039\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 14.65482119\n",
      "Iteration 2, loss = 35.12909240\n",
      "Iteration 3, loss = 36.46016242\n",
      "Iteration 4, loss = 25.16054222\n",
      "Iteration 5, loss = 16.94213728\n",
      "Iteration 6, loss = 12.52750258\n",
      "Iteration 7, loss = 11.64729941\n",
      "Iteration 8, loss = 12.21271039\n",
      "Iteration 9, loss = 12.17265545\n",
      "Iteration 10, loss = 11.62089038\n",
      "Iteration 11, loss = 10.67233667\n",
      "Iteration 12, loss = 9.47248783\n",
      "Iteration 13, loss = 8.71164801\n",
      "Iteration 14, loss = 8.37403689\n",
      "Iteration 15, loss = 7.84410214\n",
      "Iteration 16, loss = 7.60777527\n",
      "Iteration 17, loss = 7.69670517\n",
      "Iteration 18, loss = 7.59270204\n",
      "Iteration 19, loss = 7.21266546\n",
      "Iteration 20, loss = 6.96098689\n",
      "Iteration 21, loss = 7.23637745\n",
      "Iteration 22, loss = 7.26314277\n",
      "Iteration 23, loss = 6.87662154\n",
      "Iteration 24, loss = 6.74912515\n",
      "Iteration 25, loss = 6.76986462\n",
      "Iteration 26, loss = 6.78806475\n",
      "Iteration 27, loss = 6.80310436\n",
      "Iteration 28, loss = 6.78187434\n",
      "Iteration 29, loss = 6.67072506\n",
      "Iteration 30, loss = 6.87809791\n",
      "Iteration 31, loss = 7.41508916\n",
      "Iteration 32, loss = 7.42613188\n",
      "Iteration 33, loss = 7.21021679\n",
      "Iteration 34, loss = 7.38318451\n",
      "Iteration 35, loss = 8.05417266\n",
      "Iteration 36, loss = 8.65499998\n",
      "Iteration 37, loss = 8.30167446\n",
      "Iteration 38, loss = 7.79071465\n",
      "Iteration 39, loss = 7.33402357\n",
      "Iteration 40, loss = 7.13575588\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 14.60735436\n",
      "Iteration 2, loss = 35.36652051\n",
      "Iteration 3, loss = 36.80162701\n",
      "Iteration 4, loss = 25.38313080\n",
      "Iteration 5, loss = 16.97265145\n",
      "Iteration 6, loss = 12.36229006\n",
      "Iteration 7, loss = 11.41140847\n",
      "Iteration 8, loss = 12.13847614\n",
      "Iteration 9, loss = 11.94162708\n",
      "Iteration 10, loss = 11.22993584\n",
      "Iteration 11, loss = 10.54441107\n",
      "Iteration 12, loss = 9.61629125\n",
      "Iteration 13, loss = 8.77489841\n",
      "Iteration 14, loss = 8.23648940\n",
      "Iteration 15, loss = 7.82545991\n",
      "Iteration 16, loss = 7.61179240\n",
      "Iteration 17, loss = 7.48597213\n",
      "Iteration 18, loss = 7.22082264\n",
      "Iteration 19, loss = 7.14860281\n",
      "Iteration 20, loss = 7.73165029\n",
      "Iteration 21, loss = 9.00807932\n",
      "Iteration 22, loss = 9.05924773\n",
      "Iteration 23, loss = 8.21131701\n",
      "Iteration 24, loss = 7.59120694\n",
      "Iteration 25, loss = 7.26783149\n",
      "Iteration 26, loss = 7.21631554\n",
      "Iteration 27, loss = 7.22771220\n",
      "Iteration 28, loss = 7.31652405\n",
      "Iteration 29, loss = 7.17677863\n",
      "Iteration 30, loss = 7.05557722\n",
      "Iteration 31, loss = 7.12363375\n",
      "Iteration 32, loss = 6.89222092\n",
      "Iteration 33, loss = 6.87805274\n",
      "Iteration 34, loss = 7.01982181\n",
      "Iteration 35, loss = 6.88970253\n",
      "Iteration 36, loss = 6.80275983\n",
      "Iteration 37, loss = 6.92057023\n",
      "Iteration 38, loss = 7.02268762\n",
      "Iteration 39, loss = 7.05928693\n",
      "Iteration 40, loss = 7.05008740\n",
      "Iteration 41, loss = 7.22425904\n",
      "Iteration 42, loss = 7.20669164\n",
      "Iteration 43, loss = 7.07792030\n",
      "Iteration 44, loss = 7.02503094\n",
      "Iteration 45, loss = 6.94972466\n",
      "Iteration 46, loss = 6.92203945\n",
      "Iteration 47, loss = 6.80776795\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.01217942\n",
      "Iteration 2, loss = 254.17282014\n",
      "Iteration 3, loss = 406.01589699\n",
      "Iteration 4, loss = 538.02308861\n",
      "Iteration 5, loss = 598.62404396\n",
      "Iteration 6, loss = 567.75203168\n",
      "Iteration 7, loss = 491.14644577\n",
      "Iteration 8, loss = 410.74413280\n",
      "Iteration 9, loss = 342.80819893\n",
      "Iteration 10, loss = 281.60187962\n",
      "Iteration 11, loss = 221.43008624\n",
      "Iteration 12, loss = 167.69743851\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.45266535\n",
      "Iteration 2, loss = 50.31575024\n",
      "Iteration 3, loss = 51.99869812\n",
      "Iteration 4, loss = 34.54931819\n",
      "Iteration 5, loss = 22.06053650\n",
      "Iteration 6, loss = 15.71049694\n",
      "Iteration 7, loss = 14.52800863\n",
      "Iteration 8, loss = 15.52249557\n",
      "Iteration 9, loss = 14.91352928\n",
      "Iteration 10, loss = 13.81763033\n",
      "Iteration 11, loss = 12.75467075\n",
      "Iteration 12, loss = 11.05324157\n",
      "Iteration 13, loss = 9.79768100\n",
      "Iteration 14, loss = 9.31219589\n",
      "Iteration 15, loss = 8.90267321\n",
      "Iteration 16, loss = 8.65199671\n",
      "Iteration 17, loss = 8.60398651\n",
      "Iteration 18, loss = 8.19846369\n",
      "Iteration 19, loss = 7.69043640\n",
      "Iteration 20, loss = 7.48596577\n",
      "Iteration 21, loss = 7.53446502\n",
      "Iteration 22, loss = 7.90970407\n",
      "Iteration 23, loss = 7.72344991\n",
      "Iteration 24, loss = 7.12821663\n",
      "Iteration 25, loss = 6.89127600\n",
      "Iteration 26, loss = 7.15290762\n",
      "Iteration 27, loss = 7.47193367\n",
      "Iteration 28, loss = 7.28553279\n",
      "Iteration 29, loss = 7.22693289\n",
      "Iteration 30, loss = 7.54284469\n",
      "Iteration 31, loss = 7.51128319\n",
      "Iteration 32, loss = 7.52358392\n",
      "Iteration 33, loss = 7.90346950\n",
      "Iteration 34, loss = 7.94204644\n",
      "Iteration 35, loss = 7.70186653\n",
      "Iteration 36, loss = 7.89229944\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.08782354\n",
      "Iteration 2, loss = 49.60517496\n",
      "Iteration 3, loss = 51.18414492\n",
      "Iteration 4, loss = 34.25651577\n",
      "Iteration 5, loss = 22.19593248\n",
      "Iteration 6, loss = 15.71687365\n",
      "Iteration 7, loss = 14.06103511\n",
      "Iteration 8, loss = 15.03353086\n",
      "Iteration 9, loss = 14.84513802\n",
      "Iteration 10, loss = 13.62009498\n",
      "Iteration 11, loss = 12.39636827\n",
      "Iteration 12, loss = 11.20060821\n",
      "Iteration 13, loss = 10.76114968\n",
      "Iteration 14, loss = 10.09146849\n",
      "Iteration 15, loss = 9.00433028\n",
      "Iteration 16, loss = 8.44327474\n",
      "Iteration 17, loss = 8.37859926\n",
      "Iteration 18, loss = 8.19672799\n",
      "Iteration 19, loss = 7.94065664\n",
      "Iteration 20, loss = 7.93168482\n",
      "Iteration 21, loss = 7.84143652\n",
      "Iteration 22, loss = 7.69815108\n",
      "Iteration 23, loss = 7.56138241\n",
      "Iteration 24, loss = 7.38111857\n",
      "Iteration 25, loss = 7.22093298\n",
      "Iteration 26, loss = 7.41706113\n",
      "Iteration 27, loss = 7.92479173\n",
      "Iteration 28, loss = 8.13508778\n",
      "Iteration 29, loss = 7.92134694\n",
      "Iteration 30, loss = 7.44992550\n",
      "Iteration 31, loss = 7.24531397\n",
      "Iteration 32, loss = 7.90547988\n",
      "Iteration 33, loss = 8.62908971\n",
      "Iteration 34, loss = 8.61188593\n",
      "Iteration 35, loss = 7.88508324\n",
      "Iteration 36, loss = 7.46355568\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 19.00795185\n",
      "Iteration 2, loss = 48.70121207\n",
      "Iteration 3, loss = 49.63179971\n",
      "Iteration 4, loss = 32.83325510\n",
      "Iteration 5, loss = 21.32571428\n",
      "Iteration 6, loss = 15.52185099\n",
      "Iteration 7, loss = 14.28824456\n",
      "Iteration 8, loss = 15.21078906\n",
      "Iteration 9, loss = 14.98722893\n",
      "Iteration 10, loss = 14.33837491\n",
      "Iteration 11, loss = 13.74610439\n",
      "Iteration 12, loss = 12.12326404\n",
      "Iteration 13, loss = 10.62855912\n",
      "Iteration 14, loss = 9.82896818\n",
      "Iteration 15, loss = 9.36131313\n",
      "Iteration 16, loss = 9.52071326\n",
      "Iteration 17, loss = 9.66999561\n",
      "Iteration 18, loss = 9.09373878\n",
      "Iteration 19, loss = 8.26719492\n",
      "Iteration 20, loss = 7.96081760\n",
      "Iteration 21, loss = 7.99827349\n",
      "Iteration 22, loss = 8.04618944\n",
      "Iteration 23, loss = 8.00790596\n",
      "Iteration 24, loss = 7.61630695\n",
      "Iteration 25, loss = 7.35779989\n",
      "Iteration 26, loss = 7.34789070\n",
      "Iteration 27, loss = 7.28233935\n",
      "Iteration 28, loss = 7.52907418\n",
      "Iteration 29, loss = 7.68874490\n",
      "Iteration 30, loss = 7.43698097\n",
      "Iteration 31, loss = 7.28030287\n",
      "Iteration 32, loss = 7.20791652\n",
      "Iteration 33, loss = 7.38885512\n",
      "Iteration 34, loss = 7.42220353\n",
      "Iteration 35, loss = 7.23567908\n",
      "Iteration 36, loss = 7.36768081\n",
      "Iteration 37, loss = 8.07450939\n",
      "Iteration 38, loss = 8.54840922\n",
      "Iteration 39, loss = 8.34825607\n",
      "Iteration 40, loss = 7.94933251\n",
      "Iteration 41, loss = 8.15161394\n",
      "Iteration 42, loss = 8.56733592\n",
      "Iteration 43, loss = 8.24352777\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.10292692\n",
      "Iteration 2, loss = 338.01178554\n",
      "Iteration 3, loss = 546.20644203\n",
      "Iteration 4, loss = 726.20632804\n",
      "Iteration 5, loss = 807.64914857\n",
      "Iteration 6, loss = 765.05280717\n",
      "Iteration 7, loss = 660.79122065\n",
      "Iteration 8, loss = 551.65020966\n",
      "Iteration 9, loss = 459.63848337\n",
      "Iteration 10, loss = 377.21538548\n",
      "Iteration 11, loss = 296.69174247\n",
      "Iteration 12, loss = 224.70532414\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 24.00984791\n",
      "Iteration 2, loss = 65.52433713\n",
      "Iteration 3, loss = 67.93893906\n",
      "Iteration 4, loss = 44.69671554\n",
      "Iteration 5, loss = 28.01081247\n",
      "Iteration 6, loss = 19.29376711\n",
      "Iteration 7, loss = 17.05428736\n",
      "Iteration 8, loss = 18.30577659\n",
      "Iteration 9, loss = 17.89053371\n",
      "Iteration 10, loss = 16.36849199\n",
      "Iteration 11, loss = 14.62881134\n",
      "Iteration 12, loss = 12.53015131\n",
      "Iteration 13, loss = 11.06827459\n",
      "Iteration 14, loss = 10.18768099\n",
      "Iteration 15, loss = 9.44547248\n",
      "Iteration 16, loss = 9.11211720\n",
      "Iteration 17, loss = 8.86548702\n",
      "Iteration 18, loss = 8.71834391\n",
      "Iteration 19, loss = 8.60733583\n",
      "Iteration 20, loss = 7.96453813\n",
      "Iteration 21, loss = 7.72205643\n",
      "Iteration 22, loss = 8.13578047\n",
      "Iteration 23, loss = 8.03814182\n",
      "Iteration 24, loss = 7.64657870\n",
      "Iteration 25, loss = 7.30201372\n",
      "Iteration 26, loss = 7.23641057\n",
      "Iteration 27, loss = 8.06501499\n",
      "Iteration 28, loss = 9.62108348\n",
      "Iteration 29, loss = 9.49873070\n",
      "Iteration 30, loss = 7.96240861\n",
      "Iteration 31, loss = 7.46951687\n",
      "Iteration 32, loss = 8.94015864\n",
      "Iteration 33, loss = 9.72647455\n",
      "Iteration 34, loss = 9.40433921\n",
      "Iteration 35, loss = 8.66587438\n",
      "Iteration 36, loss = 8.28832719\n",
      "Iteration 37, loss = 8.73009290\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 23.52881886\n",
      "Iteration 2, loss = 64.42844817\n",
      "Iteration 3, loss = 66.94414819\n",
      "Iteration 4, loss = 44.36562455\n",
      "Iteration 5, loss = 28.11880102\n",
      "Iteration 6, loss = 19.14026995\n",
      "Iteration 7, loss = 16.70236847\n",
      "Iteration 8, loss = 18.06724235\n",
      "Iteration 9, loss = 18.14966568\n",
      "Iteration 10, loss = 17.11945081\n",
      "Iteration 11, loss = 15.97521787\n",
      "Iteration 12, loss = 14.36136988\n",
      "Iteration 13, loss = 13.17372083\n",
      "Iteration 14, loss = 12.13773137\n",
      "Iteration 15, loss = 10.81412181\n",
      "Iteration 16, loss = 10.54688019\n",
      "Iteration 17, loss = 10.81191058\n",
      "Iteration 18, loss = 10.19065069\n",
      "Iteration 19, loss = 9.38790658\n",
      "Iteration 20, loss = 8.73294327\n",
      "Iteration 21, loss = 8.43969193\n",
      "Iteration 22, loss = 8.27880132\n",
      "Iteration 23, loss = 8.10688473\n",
      "Iteration 24, loss = 7.99540109\n",
      "Iteration 25, loss = 7.85049028\n",
      "Iteration 26, loss = 8.07440153\n",
      "Iteration 27, loss = 8.11789516\n",
      "Iteration 28, loss = 7.76451491\n",
      "Iteration 29, loss = 8.00606623\n",
      "Iteration 30, loss = 8.22040731\n",
      "Iteration 31, loss = 7.98089662\n",
      "Iteration 32, loss = 7.59474713\n",
      "Iteration 33, loss = 7.55457869\n",
      "Iteration 34, loss = 7.86704323\n",
      "Iteration 35, loss = 8.17913537\n",
      "Iteration 36, loss = 8.37883319\n",
      "Iteration 37, loss = 8.13739922\n",
      "Iteration 38, loss = 8.09308402\n",
      "Iteration 39, loss = 8.52057735\n",
      "Iteration 40, loss = 8.22846944\n",
      "Iteration 41, loss = 7.68502296\n",
      "Iteration 42, loss = 7.60672995\n",
      "Iteration 43, loss = 8.18636836\n",
      "Iteration 44, loss = 8.64672870\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 23.42031359\n",
      "Iteration 2, loss = 62.98313923\n",
      "Iteration 3, loss = 64.44454930\n",
      "Iteration 4, loss = 42.07712108\n",
      "Iteration 5, loss = 26.69630036\n",
      "Iteration 6, loss = 18.88501963\n",
      "Iteration 7, loss = 16.99490735\n",
      "Iteration 8, loss = 18.15651741\n",
      "Iteration 9, loss = 17.76378460\n",
      "Iteration 10, loss = 16.30500964\n",
      "Iteration 11, loss = 14.59628807\n",
      "Iteration 12, loss = 12.66725648\n",
      "Iteration 13, loss = 11.56908665\n",
      "Iteration 14, loss = 10.87626483\n",
      "Iteration 15, loss = 10.05640744\n",
      "Iteration 16, loss = 10.16033873\n",
      "Iteration 17, loss = 10.22818951\n",
      "Iteration 18, loss = 9.51908636\n",
      "Iteration 19, loss = 9.02300662\n",
      "Iteration 20, loss = 8.95766327\n",
      "Iteration 21, loss = 8.88630780\n",
      "Iteration 22, loss = 8.29681129\n",
      "Iteration 23, loss = 7.91825205\n",
      "Iteration 24, loss = 8.07112564\n",
      "Iteration 25, loss = 8.07418878\n",
      "Iteration 26, loss = 7.85385216\n",
      "Iteration 27, loss = 8.24559267\n",
      "Iteration 28, loss = 9.35574680\n",
      "Iteration 29, loss = 9.92376678\n",
      "Iteration 30, loss = 9.50593513\n",
      "Iteration 31, loss = 8.75721803\n",
      "Iteration 32, loss = 9.17467639\n",
      "Iteration 33, loss = 9.73526451\n",
      "Iteration 34, loss = 9.40774355\n",
      "Iteration 35, loss = 8.71571868\n",
      "Iteration 36, loss = 8.49426527\n",
      "Iteration 37, loss = 8.77491231\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.17588408\n",
      "Iteration 2, loss = 421.91234261\n",
      "Iteration 3, loss = 691.08078635\n",
      "Iteration 4, loss = 928.25696456\n",
      "Iteration 5, loss = 1040.90964617\n",
      "Iteration 6, loss = 995.13993196\n",
      "Iteration 7, loss = 868.05476946\n",
      "Iteration 8, loss = 730.62843448\n",
      "Iteration 9, loss = 611.07371355\n",
      "Iteration 10, loss = 501.10173081\n",
      "Iteration 11, loss = 392.49074299\n",
      "Iteration 12, loss = 294.87106795\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 28.54058414\n",
      "Iteration 2, loss = 75.78143840\n",
      "Iteration 3, loss = 74.41867149\n",
      "Iteration 4, loss = 45.63825023\n",
      "Iteration 5, loss = 28.02233922\n",
      "Iteration 6, loss = 20.75293352\n",
      "Iteration 7, loss = 19.75590459\n",
      "Iteration 8, loss = 21.56487035\n",
      "Iteration 9, loss = 20.53676296\n",
      "Iteration 10, loss = 18.12846920\n",
      "Iteration 11, loss = 16.12951765\n",
      "Iteration 12, loss = 13.96098808\n",
      "Iteration 13, loss = 13.07173245\n",
      "Iteration 14, loss = 12.43078633\n",
      "Iteration 15, loss = 11.23480029\n",
      "Iteration 16, loss = 11.13680037\n",
      "Iteration 17, loss = 11.13982998\n",
      "Iteration 18, loss = 10.08422625\n",
      "Iteration 19, loss = 9.12664164\n",
      "Iteration 20, loss = 9.15932683\n",
      "Iteration 21, loss = 9.77834833\n",
      "Iteration 22, loss = 10.40813886\n",
      "Iteration 23, loss = 10.12250882\n",
      "Iteration 24, loss = 9.19752700\n",
      "Iteration 25, loss = 8.44602434\n",
      "Iteration 26, loss = 8.97069814\n",
      "Iteration 27, loss = 10.16756712\n",
      "Iteration 28, loss = 10.12103695\n",
      "Iteration 29, loss = 9.29233047\n",
      "Iteration 30, loss = 9.00034346\n",
      "Iteration 31, loss = 9.17953806\n",
      "Iteration 32, loss = 9.08549530\n",
      "Iteration 33, loss = 8.80444976\n",
      "Iteration 34, loss = 8.66118545\n",
      "Iteration 35, loss = 8.58393264\n",
      "Iteration 36, loss = 8.75030707\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 27.95083268\n",
      "Iteration 2, loss = 78.45610611\n",
      "Iteration 3, loss = 81.51603378\n",
      "Iteration 4, loss = 53.86816074\n",
      "Iteration 5, loss = 33.96143900\n",
      "Iteration 6, loss = 23.08385890\n",
      "Iteration 7, loss = 19.75416459\n",
      "Iteration 8, loss = 21.25435280\n",
      "Iteration 9, loss = 21.11953023\n",
      "Iteration 10, loss = 19.45911359\n",
      "Iteration 11, loss = 17.44297725\n",
      "Iteration 12, loss = 15.22837052\n",
      "Iteration 13, loss = 13.98376667\n",
      "Iteration 14, loss = 13.20331406\n",
      "Iteration 15, loss = 11.83391696\n",
      "Iteration 16, loss = 10.83824659\n",
      "Iteration 17, loss = 10.28380182\n",
      "Iteration 18, loss = 9.98686388\n",
      "Iteration 19, loss = 10.12896613\n",
      "Iteration 20, loss = 9.74107365\n",
      "Iteration 21, loss = 9.01886576\n",
      "Iteration 22, loss = 8.85506955\n",
      "Iteration 23, loss = 8.84641878\n",
      "Iteration 24, loss = 8.95300047\n",
      "Iteration 25, loss = 9.40978893\n",
      "Iteration 26, loss = 9.34454001\n",
      "Iteration 27, loss = 8.77716201\n",
      "Iteration 28, loss = 8.78192747\n",
      "Iteration 29, loss = 8.89110824\n",
      "Iteration 30, loss = 8.76577853\n",
      "Iteration 31, loss = 8.68610631\n",
      "Iteration 32, loss = 8.72650303\n",
      "Iteration 33, loss = 8.73288488\n",
      "Iteration 34, loss = 8.99566589\n",
      "Iteration 35, loss = 9.38455471\n",
      "Iteration 36, loss = 9.77034876\n",
      "Iteration 37, loss = 9.74291841\n",
      "Iteration 38, loss = 9.07129075\n",
      "Iteration 39, loss = 8.47760829\n",
      "Iteration 40, loss = 8.22677975\n",
      "Iteration 41, loss = 8.65281646\n",
      "Iteration 42, loss = 9.44834595\n",
      "Iteration 43, loss = 9.05122155\n",
      "Iteration 44, loss = 8.36993644\n",
      "Iteration 45, loss = 8.62690083\n",
      "Iteration 46, loss = 8.95352052\n",
      "Iteration 47, loss = 8.77831093\n",
      "Iteration 48, loss = 8.99754629\n",
      "Iteration 49, loss = 9.71197948\n",
      "Iteration 50, loss = 9.14792475\n",
      "Iteration 51, loss = 8.20712929\n",
      "Iteration 52, loss = 8.21149602\n",
      "Iteration 53, loss = 8.55935100\n",
      "Iteration 54, loss = 8.54990730\n",
      "Iteration 55, loss = 8.63003891\n",
      "Iteration 56, loss = 8.86070140\n",
      "Iteration 57, loss = 10.01649760\n",
      "Iteration 58, loss = 9.97870248\n",
      "Iteration 59, loss = 8.91432884\n",
      "Iteration 60, loss = 8.48907647\n",
      "Iteration 61, loss = 9.44967399\n",
      "Iteration 62, loss = 10.22251452\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 27.82091732\n",
      "Iteration 2, loss = 77.44039550\n",
      "Iteration 3, loss = 79.20803787\n",
      "Iteration 4, loss = 51.11784278\n",
      "Iteration 5, loss = 31.76320560\n",
      "Iteration 6, loss = 22.00472571\n",
      "Iteration 7, loss = 19.63730679\n",
      "Iteration 8, loss = 20.95953578\n",
      "Iteration 9, loss = 20.52062036\n",
      "Iteration 10, loss = 19.35627993\n",
      "Iteration 11, loss = 17.95267075\n",
      "Iteration 12, loss = 15.55035207\n",
      "Iteration 13, loss = 13.91017248\n",
      "Iteration 14, loss = 12.87614441\n",
      "Iteration 15, loss = 11.82510274\n",
      "Iteration 16, loss = 11.22735285\n",
      "Iteration 17, loss = 10.43932271\n",
      "Iteration 18, loss = 10.08835720\n",
      "Iteration 19, loss = 10.03973254\n",
      "Iteration 20, loss = 9.39753049\n",
      "Iteration 21, loss = 8.71905943\n",
      "Iteration 22, loss = 8.57610815\n",
      "Iteration 23, loss = 8.51902652\n",
      "Iteration 24, loss = 8.49568856\n",
      "Iteration 25, loss = 8.91313534\n",
      "Iteration 26, loss = 9.80943856\n",
      "Iteration 27, loss = 9.92765592\n",
      "Iteration 28, loss = 9.15093052\n",
      "Iteration 29, loss = 8.77797716\n",
      "Iteration 30, loss = 8.93050230\n",
      "Iteration 31, loss = 9.38827739\n",
      "Iteration 32, loss = 10.08343492\n",
      "Iteration 33, loss = 10.53020885\n",
      "Iteration 34, loss = 9.65697391\n",
      "Iteration 35, loss = 9.00828824\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 3.39084354\n",
      "Iteration 2, loss = 505.83205445\n",
      "Iteration 3, loss = 799.87572196\n",
      "Iteration 4, loss = 1045.27785807\n",
      "Iteration 5, loss = 1146.02615070\n",
      "Iteration 6, loss = 1068.31557854\n",
      "Iteration 7, loss = 906.25342816\n",
      "Iteration 8, loss = 744.34875763\n",
      "Iteration 9, loss = 613.98838157\n",
      "Iteration 10, loss = 501.62238315\n",
      "Iteration 11, loss = 393.61623737\n",
      "Iteration 12, loss = 298.26960477\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 33.11164092\n",
      "Iteration 2, loss = 95.04038846\n",
      "Iteration 3, loss = 98.63049581\n",
      "Iteration 4, loss = 63.82317372\n",
      "Iteration 5, loss = 38.81907247\n",
      "Iteration 6, loss = 25.82786303\n",
      "Iteration 7, loss = 22.55053833\n",
      "Iteration 8, loss = 24.21126047\n",
      "Iteration 9, loss = 23.37825228\n",
      "Iteration 10, loss = 21.46710800\n",
      "Iteration 11, loss = 19.84659175\n",
      "Iteration 12, loss = 16.87549759\n",
      "Iteration 13, loss = 14.40787247\n",
      "Iteration 14, loss = 13.13127150\n",
      "Iteration 15, loss = 11.87409911\n",
      "Iteration 16, loss = 11.16318802\n",
      "Iteration 17, loss = 11.16359672\n",
      "Iteration 18, loss = 11.11918266\n",
      "Iteration 19, loss = 10.82006175\n",
      "Iteration 20, loss = 9.87550658\n",
      "Iteration 21, loss = 9.05224763\n",
      "Iteration 22, loss = 9.43876438\n",
      "Iteration 23, loss = 9.66598657\n",
      "Iteration 24, loss = 9.65491870\n",
      "Iteration 25, loss = 9.56545067\n",
      "Iteration 26, loss = 9.03784502\n",
      "Iteration 27, loss = 8.61262865\n",
      "Iteration 28, loss = 9.07910619\n",
      "Iteration 29, loss = 9.25818675\n",
      "Iteration 30, loss = 8.59939305\n",
      "Iteration 31, loss = 8.32741806\n",
      "Iteration 32, loss = 8.45108159\n",
      "Iteration 33, loss = 8.88355613\n",
      "Iteration 34, loss = 9.18017638\n",
      "Iteration 35, loss = 9.24979944\n",
      "Iteration 36, loss = 9.50462976\n",
      "Iteration 37, loss = 9.24377286\n",
      "Iteration 38, loss = 9.82616837\n",
      "Iteration 39, loss = 10.10576432\n",
      "Iteration 40, loss = 9.46193108\n",
      "Iteration 41, loss = 9.05061663\n",
      "Iteration 42, loss = 9.30513024\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 32.38507191\n",
      "Iteration 2, loss = 94.22779909\n",
      "Iteration 3, loss = 98.44345278\n",
      "Iteration 4, loss = 64.49590921\n",
      "Iteration 5, loss = 39.70850558\n",
      "Iteration 6, loss = 26.07732202\n",
      "Iteration 7, loss = 22.65281176\n",
      "Iteration 8, loss = 24.68729748\n",
      "Iteration 9, loss = 23.89190893\n",
      "Iteration 10, loss = 21.64954653\n",
      "Iteration 11, loss = 19.22975210\n",
      "Iteration 12, loss = 16.42690011\n",
      "Iteration 13, loss = 14.84628459\n",
      "Iteration 14, loss = 13.79404031\n",
      "Iteration 15, loss = 12.53915839\n",
      "Iteration 16, loss = 11.61278046\n",
      "Iteration 17, loss = 10.84115479\n",
      "Iteration 18, loss = 10.62860478\n",
      "Iteration 19, loss = 10.63775675\n",
      "Iteration 20, loss = 10.24092104\n",
      "Iteration 21, loss = 9.41009302\n",
      "Iteration 22, loss = 8.85761159\n",
      "Iteration 23, loss = 8.99592652\n",
      "Iteration 24, loss = 10.01339718\n",
      "Iteration 25, loss = 10.07977523\n",
      "Iteration 26, loss = 9.14198296\n",
      "Iteration 27, loss = 9.30772776\n",
      "Iteration 28, loss = 10.54967346\n",
      "Iteration 29, loss = 11.08563516\n",
      "Iteration 30, loss = 10.37917281\n",
      "Iteration 31, loss = 9.63549209\n",
      "Iteration 32, loss = 9.74290449\n",
      "Iteration 33, loss = 9.89809325\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 32.22172915\n",
      "Iteration 2, loss = 92.78366087\n",
      "Iteration 3, loss = 95.99666081\n",
      "Iteration 4, loss = 62.15073110\n",
      "Iteration 5, loss = 38.28054901\n",
      "Iteration 6, loss = 25.60301037\n",
      "Iteration 7, loss = 22.46839414\n",
      "Iteration 8, loss = 24.41976475\n",
      "Iteration 9, loss = 23.92365704\n",
      "Iteration 10, loss = 21.60403895\n",
      "Iteration 11, loss = 19.22025512\n",
      "Iteration 12, loss = 16.40992952\n",
      "Iteration 13, loss = 14.49386141\n",
      "Iteration 14, loss = 13.43632250\n",
      "Iteration 15, loss = 12.32638461\n",
      "Iteration 16, loss = 11.74389501\n",
      "Iteration 17, loss = 11.61262637\n",
      "Iteration 18, loss = 11.89362730\n",
      "Iteration 19, loss = 11.42518303\n",
      "Iteration 20, loss = 10.04508503\n",
      "Iteration 21, loss = 9.23934899\n",
      "Iteration 22, loss = 9.56674852\n",
      "Iteration 23, loss = 9.83109442\n",
      "Iteration 24, loss = 9.64996651\n",
      "Iteration 25, loss = 9.24392069\n",
      "Iteration 26, loss = 9.67308622\n",
      "Iteration 27, loss = 9.80435182\n",
      "Iteration 28, loss = 9.31918251\n",
      "Iteration 29, loss = 8.81103448\n",
      "Iteration 30, loss = 8.81384275\n",
      "Iteration 31, loss = 9.38935611\n",
      "Iteration 32, loss = 9.93397814\n",
      "Iteration 33, loss = 9.94370242\n",
      "Iteration 34, loss = 9.78789128\n",
      "Iteration 35, loss = 10.05193109\n",
      "Iteration 36, loss = 10.01038351\n",
      "Iteration 37, loss = 9.59892763\n",
      "Iteration 38, loss = 9.55492093\n",
      "Iteration 39, loss = 10.02485101\n",
      "Iteration 40, loss = 10.21698813\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "seed = 1  \n",
    "np.random.seed(seed)\n",
    "\n",
    "neuron_number = [(i,) for i in range(5,255, 5)]\n",
    "\n",
    "param_grid = dict(hidden_layer_sizes = neuron_number)\n",
    "print(param_grid,'\\n')\n",
    "\n",
    "my_ANN_grid = MLPClassifier(hidden_layer_sizes=(30,), activation= 'logistic', \n",
    "                       solver='adam', alpha=1, random_state=1, \n",
    "                       learning_rate_init = 0.02, verbose=True, tol=0.0001)\n",
    "\n",
    "grid = GridSearchCV(my_ANN_grid, param_grid, cv=4, scoring='accuracy')\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f49eef1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fpr2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-55a214d86082>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#plt.plot(fpr1, tpr1, color='red', lw=2,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#         label='Bagging ROC Curve (area = %0.2f)' % AUC)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m plt.plot(fpr2, tpr2, color='blue', lw=2, \n\u001b[0m\u001b[0;32m      9\u001b[0m          label='ADABoost ROC Curve (area = %0.2f)' % ada_AUC)\n\u001b[0;32m     10\u001b[0m plt.plot(fpr3, tpr3, color='green', lw=2, \n",
      "\u001b[1;31mNameError\u001b[0m: name 'fpr2' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "\n",
    "# Roc Curve:\n",
    "#plt.plot(fpr, tpr, color='purple', lw=2, \n",
    "#         label='Decision tree ROC Curve (area = %0.2f)' % dt_AUC)\n",
    "#plt.plot(fpr1, tpr1, color='red', lw=2, \n",
    "#         label='Bagging ROC Curve (area = %0.2f)' % AUC)\n",
    "plt.plot(fpr2, tpr2, color='blue', lw=2, \n",
    "         label='ADABoost ROC Curve (area = %0.2f)' % ada_AUC)\n",
    "plt.plot(fpr3, tpr3, color='green', lw=2, \n",
    "         label='XGBoost ROC Curve (area = %0.2f)' % xg_AUC)\n",
    "plt.plot(fpr4, tpr4, color='black', lw=2, \n",
    "         label='Random Forest ROC Curve (area = %0.2f)' % rf_AUC)\n",
    "\n",
    "# Random Guess line:\n",
    "plt.plot([0, 1], [0, 1], color='blue', lw=1, linestyle='--')\n",
    "\n",
    "\n",
    "# Defining The Range of X-Axis and Y-Axis:\n",
    "plt.xlim([-0.005, 1.005])\n",
    "plt.ylim([0.0, 1.01])\n",
    "\n",
    "# Labels, Title, Legend:\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1efb77fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#everything under this is a test to see if we could run TensorFlow with our array, it does not work due to our data being too\n",
    "#large\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac4b6a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f9fb731",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.67 TiB for an array with shape (345, 2125715229) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0cf849a6687a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclass_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m   \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m   \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m   \u001b[0mcategorical\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m   \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.67 TiB for an array with shape (345, 2125715229) and data type float32"
     ]
    }
   ],
   "source": [
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "class_num = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2987c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97b8b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3), input_shape=X_train.shape[1:], padding='same'))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c857b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (3, 3), input_shape=(3, 32, 32), activation='relu', padding='same'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952099c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea62c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f5317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bbda19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f0e9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "    \n",
    "model.add(Conv2D(128, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faac55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23243e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(256, kernel_constraint=maxnorm(3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "    \n",
    "model.add(Dense(128, kernel_constraint=maxnorm(3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69aa5d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(class_num))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138c6af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 25\n",
    "optimizer = 'adam'\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e3ab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e18e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
